---
title: "Milestone_Report"
author: "Filipp Trubin"
date: "1/17/2022"
output:
  html_document: default
  pdf_document: default
---

Intro
============
Over the past decade, there has been a dramatic increase in the usage of electronic devices for email, social networking and other activities. Errors typing on such devices are far from uncommon, and can have considerable implications concerning the efficient use of such devices for communication. 
Text prediction requires estimation of the next character or word given a string of the input history. This may represent a useful solution to the problem of mistyping words. 
In this project we aim to develop a text predictive algorithm derived from large data sets composed of different source material blogs, twitter etc. to develop an application. 


1. Data wrangling
======================================
Source: www.corpora.heliohost.org.   
Dataset contains txt files on four languages (english, german, finish, russian) of blogs, news and twitter.


**Goals:**

The main goal of this report is to do Exploratory Data Analysis of texts.


# 1. Data processing

```{r message=FALSE, warning=FALSE}
library(stringr)
library(RWeka)
library(tm)
library(wordcloud)
library(qdap)
library(ggplot2)
library(gridExtra)

options(scipen = 999)
```

Creating Datasets (for this report all the .txt files where reduced on 70% in size to provide quicker loading). 

```{r message=FALSE, warning=FALSE}
corpus.blogs <- readLines("final/en_US/en_US.blogs.txt")
nlines.blogs <- length(corpus.blogs)
nwords.blogs <- sum(unlist(sapply(corpus.blogs, str_count, pattern = " "))) + nlines.blogs
corpus.news <- readLines("final/en_US/en_US.news.txt")
nlines.news <- length(corpus.news)
nwords.news <- sum(unlist(sapply(corpus.news, str_count, pattern = " "))) + nlines.news
corpus.twitter <- readLines("final/en_US/en_US.twitter.txt")
nlines.twitter <- length(corpus.twitter)
nwords.twitter <- sum(unlist(sapply(corpus.twitter, str_count, pattern = " "))) + nlines.twitter
df.lines <- data.frame(file = factor(c("blogs", "news", "twitter")), count = c(nlines.blogs, nlines.news, nlines.twitter))
df.words <- data.frame(file = factor(c("blogs", "news", "twitter")), count = c(nwords.blogs, nwords.news, nwords.twitter))
df.comb <- data.frame(File = factor(c("Blogs", "News", "Twitter")), Count_of_lines = c(nlines.blogs, nlines.news, nlines.twitter), Count_of_words = c(nwords.blogs, nwords.news, nwords.twitter))
df
```

 
**Mean line length of *blogs* dataset is** `r round(mean(sapply(corpus.blogs, str_length)), 2)`   
**Mean line lengths of *news* dataset is** `r round(mean(sapply(corpus.news, str_length)), 2)`    
**Mean line length of *twitter* dataset is** `r round(mean(sapply(corpus.twitter, str_length)), 2)`.   


Visualizing: 


```{r}
n.word.plot <- ggplot(data = df.words, aes(x = file, y = count), environment=environment()) + 
        geom_bar(stat = "identity", colour="red", fill="blue") + 
        xlab(".txt files ") + ylab("N of words")

n.line.plot<- ggplot(data = df.lines, aes(x = file, y = count), environment=environment()) + 
        geom_bar(stat = "identity", colour="blue", fill="red") + 
        xlab(".txt files ") + ylab("N of lines")
grid.arrange(n.word.plot, n.line.plot, ncol = 2, nrow = 2)
```


# Data cleaning 
The *stringi* and *tm* packages were used for the text transformation into a compatible format for further tokenisation.

Steps:
 - Apply lower case 
 - Replace i with I
 - Clear punctuation except for ".", and "'"
 - Remove white spaces, dot spaces, numbers 
 - Split lines by the postitioning of dots
 

```{r message=FALSE, warning=FALSE}
library(stringi)
directory_source <- DirSource("final/en_US", encoding = "UTF-8")
corpus <- Corpus(directory_source,readerControl=list(reader=readPlain))
corpus.cleaned <- tm_map(corpus, function(x) stri_trans_tolower(x[[1]]))
corpus.cleaned <- tm_map(corpus.cleaned, function(x) gsub(" i ", " I ", x))
corpus.cleaned <- tm_map(corpus.cleaned, function(x) gsub(" i'", " I'", x))
corpus.cleaned <- tm_map(corpus.cleaned, function(x) gsub("[!?,.]+", ".", x))
corpus.cleaned <- tm_map(corpus.cleaned, function(x) gsub('[])(;:#%$^*\\~{}[&+=@/"`|<>_]+', "", x))
corpus.cleaned <- tm_map(corpus.cleaned, stripWhitespace)
corpus.cleaned <- tm_map(corpus.cleaned, function(x) gsub(" \\.", ".", x))
corpus.cleaned <- tm_map(corpus.cleaned, function(x) gsub("\\. ", ".", x))
corpus.cleaned <- tm_map(corpus.cleaned, removeNumbers)
corpus.cleaned <- tm_map(corpus.cleaned, function(x) strsplit(x, "\\."))
```

**Examples: BEFORE EDA**
```{r}
#head(corpus$ en_US.blogs.txt, n = 1)
df_corp <- as.data.frame(corpus)
substr(df_corp$text, 1,200)
```
**AFTER EDA:**

```{r}
df_corp_clean <- as.data.frame(corpus.cleaned)
substr(df_corp_clean$text, 1,200)
```

*Profanity filter**
Was not applied by now to provide better prediction at eh end. Will be Applied at the end of the main project.
